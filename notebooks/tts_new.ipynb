{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a413e878-8801-4022-ba86-06c4b9def1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4853/3039404210.py:5: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(audio_path)\n",
      "/home/fx/.local/lib/python3.10/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'english.wav'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/librosa/core/audio.py:176\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__soundfile_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m sf\u001b[38;5;241m.\u001b[39mSoundFileRuntimeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# If soundfile failed, try audioread instead\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/librosa/core/audio.py:209\u001b[0m, in \u001b[0;36m__soundfile_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;66;03m# Otherwise, create the soundfile object\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context \u001b[38;5;28;01mas\u001b[39;00m sf_desc:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/soundfile.py:658\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[1;32m    657\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[0;32m--> 658\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/soundfile.py:1216\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1215\u001b[0m     err \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_error(file_ptr)\n\u001b[0;32m-> 1216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LibsndfileError(err, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError opening \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode_int \u001b[38;5;241m==\u001b[39m _snd\u001b[38;5;241m.\u001b[39mSFM_WRITE:\n\u001b[1;32m   1218\u001b[0m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n",
      "\u001b[0;31mLibsndfileError\u001b[0m: Error opening 'english.wav': System error.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the audio file\u001b[39;00m\n\u001b[1;32m      4\u001b[0m audio_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish.wav\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with the actual path to your audio file\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m y, sr \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Set the start and end times for trimming\u001b[39;00m\n\u001b[1;32m      8\u001b[0m start_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Start at 0 seconds\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/librosa/core/audio.py:184\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPurePath)):\n\u001b[1;32m    181\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPySoundFile failed. Trying audioread instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    183\u001b[0m     )\n\u001b[0;32m--> 184\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__audioread_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m<decorator-gen-20>:2\u001b[0m, in \u001b[0;36m__audioread_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/librosa/util/decorators.py:60\u001b[0m, in \u001b[0;36mdeprecated.<locals>.__wrapper\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Warn the user, and then proceed.\"\"\"\u001b[39;00m\n\u001b[1;32m     52\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mDeprecated as of librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mIt will be removed in librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# Would be 2, but the decorator adds a level\u001b[39;00m\n\u001b[1;32m     59\u001b[0m )\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/librosa/core/audio.py:241\u001b[0m, in \u001b[0;36m__audioread_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    238\u001b[0m     reader \u001b[38;5;241m=\u001b[39m path\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# If the input was not an audioread object, try to open it\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m     reader \u001b[38;5;241m=\u001b[39m \u001b[43maudioread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m reader \u001b[38;5;28;01mas\u001b[39;00m input_file:\n\u001b[1;32m    244\u001b[0m     sr_native \u001b[38;5;241m=\u001b[39m input_file\u001b[38;5;241m.\u001b[39msamplerate\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/audioread/__init__.py:127\u001b[0m, in \u001b[0;36maudio_open\u001b[0;34m(path, backends)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m BackendClass \u001b[38;5;129;01min\u001b[39;00m backends:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBackendClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError:\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/audioread/rawread.py:59\u001b[0m, in \u001b[0;36mRawAudioFile.__init__\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m aifc\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'english.wav'"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "# Load the audio file\n",
    "audio_path = 'english.wav'  # Replace with the actual path to your audio file\n",
    "y, sr = librosa.load(audio_path)\n",
    "\n",
    "# Set the start and end times for trimming\n",
    "start_time = 0  # Start at 0 seconds\n",
    "end_time = 5 * 60 + 9  # 5 minutes and 17 seconds\n",
    "\n",
    "# Calculate the corresponding sample indices\n",
    "start_sample = int(start_time * sr)\n",
    "end_sample = int(end_time * sr)\n",
    "\n",
    "# Trim the audio clip\n",
    "trimmed_clip = y[start_sample:end_sample]\n",
    "\n",
    "# Save the trimmed clip to a new file\n",
    "trimmed_audio_path = 'trimmed_audio.wav'\n",
    "sf.write(trimmed_audio_path, trimmed_clip, sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34cf928b-6d7b-40a0-b7a9-37cd87c23316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No API token found for üê∏Coqui Studio voices - https://coqui.ai \n",
      "Visit üîóhttps://app.coqui.ai/account to get one.\n",
      "Set it as an environment variable `export COQUI_STUDIO_TOKEN=<token>`\n",
      "\n",
      " > Downloading model to /home/fx/.local/share/tts/tts_models--en--ljspeech--glow-tts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 344M/344M [00:59<00:00, 5.83MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Model's license - MPL\n",
      " > Check https://www.mozilla.org/en-US/MPL/2.0/ for more info.\n",
      " > Downloading model to /home/fx/.local/share/tts/vocoder_models--en--ljspeech--multiband-melgan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 82.8M/82.8M [00:11<00:00, 7.37MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Model's license - MPL\n",
      " > Check https://www.mozilla.org/en-US/MPL/2.0/ for more info.\n",
      " > Using model: glow_tts\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:0\n",
      " | > fft_size:1024\n",
      " | > power:1.1\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:50.0\n",
      " | > mel_fmax:7600.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:1.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:10\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Vocoder Model: multiband_melgan\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:0\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:True\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:50.0\n",
      " | > mel_fmax:7600.0\n",
      " | > pitch_fmin:0.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:/home/fx/.local/share/tts/vocoder_models--en--ljspeech--multiband-melgan/scale_stats.npy\n",
      " | > base:10\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Generator Model: multiband_melgan_generator\n",
      " > Discriminator Model: melgan_multiscale_discriminator\n"
     ]
    }
   ],
   "source": [
    "from TTS.api import TTS\n",
    "\n",
    "# Running a multi-speaker and multi-lingual model\n",
    "\n",
    "# List available üê∏TTS models and choose the first one\n",
    "model_name = TTS()\n",
    "model=model_name.list_models()[10]\n",
    "\n",
    "# Init TTS\n",
    "tts = TTS(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0587998-b7e3-4a33-af64-b5bfcfacf303",
   "metadata": {},
   "outputs": [],
   "source": [
    "text='''You know,  when I was learning to speak in English,  there was this problem that used to bother me a lot.  I was able to understand everything in English.  I could understand every single word a person would say to me.  But when it comes to speaking in English,  God knows what used to happen to me.  I was unable to speak in English at all.  And it would fill me with a lot of frustration.  Because here I am,  understanding every single word that somebody would say to me.  I could understand English completely.\n",
    " But when it comes to speaking in English,  the other side of the language,  I was unable to.  So there was this frustrating gap between these two poles.  I'm getting every word, but I'm unable to speak a word.  And that is something that I want to address in this video.  I want you to change your attitude towards this issue.  I do not want you to address it as a problem and be stuck with it for the rest of your life.  Okay,  here's a suggestion. And tell me if it works for you.  On a scale of 0 to 10,  at 0,  no speaking at all.  Unable to speak a word in English.  At 10,  you're fluent.  Okay.  Now, if I ask you right now,  where do you stand on the scale?  I'm pretty sure you would mark yourself, at somewhere between three and five. So either you would be standing at three or four or five,  somewhere here, right?  Talking about any other language, let's talk about Russian or German or French.  Do you understand any of these languages for the most of us?  No,  we do not.  I don't.  We don't understand these languages.  So for English,  we already have a great head start.  At least we understand the language.  You get it?  So we already have a great head start with English because we already understand the language.  So we're not starting from a zero really.  We do understand the language.  We have a certain knowledge of a few things about English.  So it is easier,  simple.  It's much easier than rest of the languages.  You want to understand this. I'm telling you,  you need to hear this from me that this is not your mother tongue.  This is not your mother tongue.  I mean, how are you supposed to be as fluent in English as you are in your mother tongue?  You could never be, I believe.  Never be.  Because mother tongue is something that we are blessed with.  We are naturally good at it. We are naturally heroes at it.  But for any other language,  you need to put in some work.  And that is what we're trying to do here.  So if you are standing somewhere on the scale of zero to ten,  and let's say you're standing at a three right now,  for example,  you're standing at a three or a three right now.  Now you cannot be a ten right away.  It doesn't work that way. It cannot work that way for any human.  In order to be a ten,\n",
    " you will have to gradually move your steps from three,  become a four,  then from four to five,  five to six,  six to seven,  seven to eight, eight to nine, nine to ten.  And that is how it is going to work for you.  So if you already do understand the language,  the next part is training yourself on speaking because speaking is a separate activity altogether and you need to put in fresh effort and train yourself from the start in order to do this properly.  Okay.  And then there are two things that are missing here that you need to start working on.  First is proactive practice.  Now, the good thing is that you have,  you already understand it.  So it's not going to be that difficult for you.  All you need to do is start proactively practicing to speak in English and to start with,  had I been in your place and I have been in your place in the past.  So what I would do is that I would allot a certain time period of my day to proactively  practice to speak in English. I'll say 15 minutes a day is a good start for you.  15 minutes a day.  Let's start with that and do it for the next 30 days.  Proactively practicing,  learning to speak in English for 15 minutes in a day.  And I have a great app for you if you want to do that.  This is my recommendation.  This is my suggestion.  You can start doing it on this app called Ingvartha.  Ingvartha is a very useful English speaking practice app.  I've known about it for quite some time and reiterating because I've already spoken about this application in a few of my previous videos.  All you've got to do is when you download the app and you make a call,  you get connected to English experts.  Now, the best part about these experts is that they do not judge you at all.  They don't judge you.  They are friendly and they will actually guide you with honest feedback.  You can let them know if you do not want them to correct you over your mistakes.  That's also an option.  Absolutely.  Now, the good part is that whenever you make a call, you get connected to these English experts and this helps you create a real world English speaking environment for yourself.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22ca5f83-7865-4a1b-8001-759a974a9178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "['You know,  when I was learning to speak in English,  there was this problem that used to bother me a lot.', 'I was able to understand everything in English.', 'I could understand every single word a person would say to me.', 'But when it comes to speaking in English,  God knows what used to happen to me.', 'I was unable to speak in English at all.', 'And it would fill me with a lot of frustration.', 'Because here I am,  understanding every single word that somebody would say to me.', 'I could understand English completely.', 'But when it comes to speaking in English,  the other side of the language,  I was unable to.', 'So there was this frustrating gap between these two poles.', \"I'm getting every word, but I'm unable to speak a word.\", 'And that is something that I want to address in this video.', 'I want you to change your attitude towards this issue.', 'I do not want you to address it as a problem and be stuck with it for the rest of your life.', \"Okay,  here's a suggestion.\", 'And tell me if it works for you.', 'On a scale of 0 to 10,  at 0,  no speaking at all.', 'Unable to speak a word in English.', \"At 10,  you're fluent.\", 'Okay.', 'Now, if I ask you right now,  where do you stand on the scale?', \"I'm pretty sure you would mark yourself, at somewhere between three and five.\", 'So either you would be standing at three or four or five,  somewhere here, right?', \"Talking about any other language, let's talk about Russian or German or French.\", 'Do you understand any of these languages for the most of us?', 'No,  we do not.', \"I don't.\", \"We don't understand these languages.\", 'So for English,  we already have a great head start.', 'At least we understand the language.', 'You get it?', 'So we already have a great head start with English because we already understand the language.', \"So we're not starting from a zero really.\", 'We do understand the language.', 'We have a certain knowledge of a few things about English.', 'So it is easier,  simple.', \"It's much easier than rest of the languages.\", 'You want to understand this.', \"I'm telling you,  you need to hear this from me that this is not your mother tongue.\", 'This is not your mother tongue.', 'I mean, how are you supposed to be as fluent in English as you are in your mother tongue?', 'You could never be, I believe.', 'Never be.', 'Because mother tongue is something that we are blessed with.', 'We are naturally good at it.', 'We are naturally heroes at it.', 'But for any other language,  you need to put in some work.', \"And that is what we're trying to do here.\", \"So if you are standing somewhere on the scale of zero to ten,  and let's say you're standing at a three right now,  for example,  you're standing at a three or a three right now.\", 'Now you cannot be a ten right away.', \"It doesn't work that way.\", 'It cannot work that way for any human.', 'In order to be a ten,', 'you will have to gradually move your steps from three,  become a four,  then from four to five,  five to six,  six to seven,  seven to eight, eight to nine, nine to ten.', 'And that is how it is going to work for you.', 'So if you already do understand the language,  the next part is training yourself on speaking because speaking is a separate activity altogether and you need to put in fresh effort and train yourself from the start in order to do this properly.', 'Okay.', 'And then there are two things that are missing here that you need to start working on.', 'First is proactive practice.', 'Now, the good thing is that you have,  you already understand it.', \"So it's not going to be that difficult for you.\", 'All you need to do is start proactively practicing to speak in English and to start with,  had I been in your place and I have been in your place in the past.', 'So what I would do is that I would allot a certain time period of my day to proactively  practice to speak in English.', \"I'll say 15 minutes a day is a good start for you.\", '15 minutes a day.', \"Let's start with that and do it for the next 30 days.\", 'Proactively practicing,  learning to speak in English for 15 minutes in a day.', 'And I have a great app for you if you want to do that.', 'This is my recommendation.', 'This is my suggestion.', 'You can start doing it on this app called Ingvartha.', 'Ingvartha is a very useful English speaking practice app.', \"I've known about it for quite some time and reiterating because I've already spoken about this application in a few of my previous videos.\", \"All you've got to do is when you download the app and you make a call,  you get connected to English experts.\", 'Now, the best part about these experts is that they do not judge you at all.', \"They don't judge you.\", 'They are friendly and they will actually guide you with honest feedback.', 'You can let them know if you do not want them to correct you over your mistakes.', \"That's also an option.\", 'Absolutely.', 'Now, the good part is that whenever you make a call, you get connected to these English experts and this helps you create a real world English speaking environment for yourself.']\n",
      "b åt w…õn …™t k åmz t…ô spik…™≈ã …™n …™≈ã…°l…™ É, √∞…ô  å√∞…ö sa…™d …ôv √∞…ô l√¶≈ã…°w…™dÕ° í, a…™ w…ôz  åne…™b…ôl t…ô.\n",
      " [!] Character 'Õ°' not found in the vocabulary. Discarding it.\n",
      " > Processing time: 13.995280265808105\n",
      " > Real-time factor: 0.03991557471180067\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'output_english.wav'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tts.tts_to_file(text, speaker_wav=\"trimmed_audio.wav\", file_path=\"output_english.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041299d1-7799-49a8-bde0-af66ca5cfcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjusted_audio = audio.speedup(playback_speed=1/1.378125)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "67093456-4c21-4c4c-8c7d-b308a9c12fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech Rate (frames/second): 16000.0\n",
      "Mean Pitch (Fundamental Frequency): 46.019714\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# Load the audio file\n",
    "audio_file = \"output_english.wav\"\n",
    "y, sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "# Compute speech rate (duration / number of frames)\n",
    "duration = librosa.get_duration(y=y, sr=sr)\n",
    "n_frames = len(y)\n",
    "speech_rate = n_frames / duration\n",
    "print(\"Speech Rate (frames/second):\", speech_rate)\n",
    "\n",
    "# Compute pitch (fundamental frequency)\n",
    "# Compute pitch (fundamental frequency)\n",
    "pitches, magnitudes = librosa.piptrack(y=y, sr=sr)\n",
    "mean_pitch = np.mean(pitches[np.argmax(magnitudes, axis=0)])\n",
    "print(\"Mean Pitch (Fundamental Frequency):\", mean_pitch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b7d1ac3c-0d16-478d-a18f-39db48f9c27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech Rate (frames/second): 22050.0\n",
      "Mean Pitch (Fundamental Frequency): 75.84873\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# Load the audio file\n",
    "audio_file = \"trimmed_audio.wav\"\n",
    "y, sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "# Compute speech rate (duration / number of frames)\n",
    "duration = librosa.get_duration(y=y, sr=sr)\n",
    "n_frames = len(y)\n",
    "speech_rate = n_frames / duration\n",
    "print(\"Speech Rate (frames/second):\", speech_rate)\n",
    "\n",
    "# Compute pitch (fundamental frequency)\n",
    "# Compute pitch (fundamental frequency)\n",
    "pitches, magnitudes = librosa.piptrack(y=y, sr=sr)\n",
    "mean_pitch = np.mean(pitches[np.argmax(magnitudes, axis=0)])\n",
    "print(\"Mean Pitch (Fundamental Frequency):\", mean_pitch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40266e80-63e5-4553-8db7-b181e4caa932",
   "metadata": {},
   "source": [
    "## We will change the speech rate of output file from 16000 to 22050.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "67898a59-56c2-4dbf-b48b-e6f034efb870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "\n",
    "# Load the audio file\n",
    "audio_path = 'output_english.wav'  # Replace with your audio file path\n",
    "audio = AudioSegment.from_file(audio_path)\n",
    "audio = audio.set_frame_rate(22050)\n",
    "# speech Rate Factor = 22050 / 16000 \n",
    "# ‚âà 1.378125\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6212a9f3-5d27-4bde-b591-3a5a7c8ddb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "play(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed7ff15-bcfb-4b40-bce3-96890e7277d8",
   "metadata": {},
   "source": [
    "## Now we will change the pitch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "476b6b23-3cfb-4d35-9c89-240f384ac996",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pyin() got an unexpected keyword argument 'n_steps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m semitones \u001b[38;5;241m=\u001b[39m desired_pitch \u001b[38;5;241m-\u001b[39m current_pitch\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Adjust the pitch of the audio file without using a function.\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m pitch_shift \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpyin\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msemitones\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Save the audio file with the adjusted pitch.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m pitch_shift\u001b[38;5;241m.\u001b[39mexport(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwav\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: pyin() got an unexpected keyword argument 'n_steps'"
     ]
    }
   ],
   "source": [
    "import pydub\n",
    "import librosa\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  audio_file = \"output_english.wav\"\n",
    "  desired_pitch = 75.84873\n",
    "  current_pitch = 46.019714\n",
    "  audio = pydub.AudioSegment.from_file(audio_file)\n",
    "  sample_rate = audio.frame_rate\n",
    "  semitones = desired_pitch - current_pitch\n",
    "  # Adjust the pitch of the audio file without using a function.\n",
    "  pitch_shift = librosa.core.pyin(audio, sr=sample_rate, n_steps=8.6505)\n",
    "\n",
    "  # Save the audio file with the adjusted pitch.\n",
    "  pitch_shift.export(\"output.wav\", format=\"wav\")\n",
    "\n",
    "  print(\"Audio file pitch adjusted to {} Hz.\".format(desired_pitch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "52143eae-1c89-4580-aebf-14eee5407208",
   "metadata": {},
   "outputs": [
    {
     "ename": "UFuncTypeError",
     "evalue": "ufunc '_phasor_angles' did not contain a loop with signature matching types <class 'numpy.dtype[float32]'> -> None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# filename = librosa.example(audio)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m y, sr \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_english.wav\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m new_y \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meffects\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpitch_shift\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8.06505\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m soundfile\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpitchShifted.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_y, sr,)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/librosa/effects.py:336\u001b[0m, in \u001b[0;36mpitch_shift\u001b[0;34m(y, sr, n_steps, bins_per_octave, res_type, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mfloat\u001b[39m(n_steps) \u001b[38;5;241m/\u001b[39m bins_per_octave)\n\u001b[1;32m    334\u001b[0m \u001b[38;5;66;03m# Stretch in time, then resample\u001b[39;00m\n\u001b[1;32m    335\u001b[0m y_shift \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mresample(\n\u001b[0;32m--> 336\u001b[0m     \u001b[43mtime_stretch\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    337\u001b[0m     orig_sr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(sr) \u001b[38;5;241m/\u001b[39m rate,\n\u001b[1;32m    338\u001b[0m     target_sr\u001b[38;5;241m=\u001b[39msr,\n\u001b[1;32m    339\u001b[0m     res_type\u001b[38;5;241m=\u001b[39mres_type,\n\u001b[1;32m    340\u001b[0m )\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m# Crop to the same dimension as the input\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m util\u001b[38;5;241m.\u001b[39mfix_length(y_shift, size\u001b[38;5;241m=\u001b[39my\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/librosa/effects.py:245\u001b[0m, in \u001b[0;36mtime_stretch\u001b[0;34m(y, rate, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m stft \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mstft(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# Stretch by phase vocoding\u001b[39;00m\n\u001b[0;32m--> 245\u001b[0m stft_stretch \u001b[38;5;241m=\u001b[39m \u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphase_vocoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhop_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn_fft\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# Predict the length of y_stretch\u001b[39;00m\n\u001b[1;32m    253\u001b[0m len_stretch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m rate))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/librosa/core/spectrum.py:1457\u001b[0m, in \u001b[0;36mphase_vocoder\u001b[0;34m(D, rate, hop_length, n_fft)\u001b[0m\n\u001b[1;32m   1454\u001b[0m mag \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m alpha) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(columns[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m+\u001b[39m alpha \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(columns[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;66;03m# Store to output array\u001b[39;00m\n\u001b[0;32m-> 1457\u001b[0m d_stretch[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, t] \u001b[38;5;241m=\u001b[39m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphasor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphase_acc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[38;5;66;03m# Compute phase advance\u001b[39;00m\n\u001b[1;32m   1460\u001b[0m dphase \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mangle(columns[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mangle(columns[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m-\u001b[39m phi_advance\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/librosa/util/utils.py:2602\u001b[0m, in \u001b[0;36mphasor\u001b[0;34m(angles, mag)\u001b[0m\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mphasor\u001b[39m(\n\u001b[1;32m   2545\u001b[0m     angles: Union[np\u001b[38;5;241m.\u001b[39mndarray, _Real],\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   2547\u001b[0m     mag: Optional[Union[np\u001b[38;5;241m.\u001b[39mndarray, _Number]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2548\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mcomplex_]:\n\u001b[1;32m   2549\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct a complex phasor representation from angles.\u001b[39;00m\n\u001b[1;32m   2550\u001b[0m \n\u001b[1;32m   2551\u001b[0m \u001b[38;5;124;03m    When `mag` is not provided, this is equivalent to:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2600\u001b[0m \u001b[38;5;124;03m    array([5.000e-01+0.j , 9.185e-17+1.5j])\u001b[39;00m\n\u001b[1;32m   2601\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2602\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[43m_phasor_angles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mangles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2604\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mag \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2605\u001b[0m         z \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m mag\n",
      "\u001b[0;31mUFuncTypeError\u001b[0m: ufunc '_phasor_angles' did not contain a loop with signature matching types <class 'numpy.dtype[float32]'> -> None"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import soundfile \n",
    "# filename = librosa.example(audio)\n",
    "y, sr = librosa.load('output_english.wav')\n",
    "new_y = librosa.effects.pitch_shift(y=y,sr=sr,n_steps=8.06505)\n",
    "soundfile.write(\"pitchShifted.wav\", new_y, sr,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792f987e-9dc3-49f6-ab13-2b9dd2022852",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
